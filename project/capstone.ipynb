{"cells":[{"metadata":{"_uuid":"06dc531a8cb411334161905e4df621ffc9705a28"},"cell_type":"markdown","source":"# Salt Identification Challenge\n## Capstone Project\nShane Moloney  \nUdacity Machine Learning Nanodegree"},{"metadata":{"_uuid":"d1575855f590cea126c825565a3f8b9849cfaf4e"},"cell_type":"markdown","source":"## Set up\n\nImports for all the project dependencies.\nI may split this so that the dependencies are in the relevant code cells to avoid needing to run this every time."},{"metadata":{"trusted":true,"_uuid":"e33c6960a4d6edf66dc23a3728f14a1049086648","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95b583d4945e07a96b93c038fb072a3842fc8e56","collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport math\n\nfrom random import randint\n\nimport cv2\nfrom pprint import pprint\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout\n\nfrom tqdm import tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4951504395c322ecfed8e96a3e16755ce28612e"},"cell_type":"markdown","source":"### Load in a Sample of the data\n\nLet's have a look at the data to see what will need to be done to prepare it for learning."},{"metadata":{"trusted":true,"_uuid":"97e6460a36278ee0784cec30c370658145f0cd0b","collapsed":true},"cell_type":"code","source":"train_samp = pd.read_csv(\"../input/train.csv\", index_col=\"id\", nrows=5)\ndepths_samp = pd.read_csv(\"../input/depths.csv\", index_col=\"id\", nrows=5)\nprint(train_samp)\nprint(depths_samp)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7918c3bb563f191728dc8be7a7600db8f51ca537"},"cell_type":"markdown","source":"Above we can see the run-length encoded masks and the depths of the first five images.  \n\nAs we can see, images with no salt deposits have a null value and images with deposits have the pixels listed as ranges that cantain salt.  \n\nNow we will look at the images themselves along with their provided masks as these are kindly provided to us in png form although they could also be extracted from the train.csv file by decoding the entries in the file."},{"metadata":{"trusted":true,"_uuid":"b60c453b64744b74e1f2ae693e3c90a198e5fa82","collapsed":true},"cell_type":"code","source":"train_inds = train_samp.index\n\nimage_samp = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in train_inds]\nmask_samp = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in train_inds]\nimage_size = image_samp[0].shape\nprint(\"Image Dimensions: {}\".format(image_size))\n\nfig, axs = plt.subplots(5, 2, figsize=(10, 25))\nfor i in range(0, 5):\n    axs[int(i)][0].imshow(image_samp[i-1], cmap='gray')\n    axs[int(i)][1].imshow(mask_samp[i-1], cmap='gray')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3737467e198ca86272417a2123979d2cebda31b"},"cell_type":"markdown","source":"We can see that some masks may not be accurate as in the figure above the mask seems to claim that the salt deposit takes up exactly half the image with a perfectly straight border, which we can clearly see is incorrect.  \n\nI may decide to remove any data whose mask matches this pattern.  \n\nFor now lets load in all the data and start looking at it as a whole."},{"metadata":{"_uuid":"d0597058dc5f1738582bf4d4ce7c356c4cbaa641"},"cell_type":"markdown","source":"## Loading in the Data"},{"metadata":{"_uuid":"853c5e18b41f39079870dfb8f6e9c4c89c7ac34a"},"cell_type":"markdown","source":"First we'll import the CSV files and add the depths as well and store a list of the indeces and a list of the test set depths, ie the depths whose ID's do not appear in the test set."},{"metadata":{"trusted":true,"_uuid":"d2d41d32f1b8a21f185b66bd7756c1c27271896a","collapsed":true},"cell_type":"code","source":"train_full = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\ndepths = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\n\ntrain_full = train_full.join(depths)\ntrain_idx = train_full.index\n\ntest_depths = depths[~depths.index.isin(train_full.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aba0bfce466e7b7d53564b4852b0124f98946654"},"cell_type":"markdown","source":"\n\nNow we import the image and mask files, 4000 in total."},{"metadata":{"trusted":true,"_uuid":"cbde9fcc69deab102739379c15c9e28919cd599f","collapsed":true},"cell_type":"code","source":"train_full[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_idx)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69ecf86245987b47b5950818ea5b89be9c358ec7","collapsed":true},"cell_type":"code","source":"train_full[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_idx)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9106b4bb1b09a89bd0377ddcabbe2d5cbbad104a"},"cell_type":"markdown","source":"Create a column in the array for the coverage of the salt in each image. This means the number of pixels xcontaining salt over the area of the image. As this can be any decimal between 0 and 1 we discretise these values with the cov_to_class function which gives it an integer class from 1 to 10 which represent no salt up to only salt respectively."},{"metadata":{"trusted":true,"_uuid":"671c24d495159f45bc8df32cc0cf7156eb2d40c0","collapsed":true},"cell_type":"code","source":"train_full[\"coverage\"] = train_full.masks.map(np.sum) / pow(image_size[0], 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4304cc595d680a096bfc314722c7df02bd8939c4","collapsed":true},"cell_type":"code","source":"def cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \ntrain_full[\"coverage_class\"] = train_full.coverage.map(cov_to_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ff24b630c56b5ea6feeb00da013d6f0e3be0696","collapsed":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(15,5))\nsns.distplot(train_full.coverage, kde=False, ax=axs[0])\nsns.distplot(train_full.coverage_class, bins=10, kde=False, ax=axs[1])\nplt.suptitle(\"Salt coverage\")\naxs[0].set_xlabel(\"Coverage\")\naxs[1].set_xlabel(\"Coverage class\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18339b8a9ef26fa33a114a3743e5e6097b5f660f"},"cell_type":"markdown","source":"We can see that a large amount of the images have no or very little salt, half the data set resides in classes 1 and 2. This is good as the model needs to know that there are images without salt as well.\n\n## Depths\nNow let's have a look at the depths and how good their random sampling was."},{"metadata":{"trusted":true,"_uuid":"e134c09e00464861dd217c4b105f713c5a69d683","collapsed":true},"cell_type":"code","source":"sns.distplot(train_full.z, label=\"Train\")\nsns.distplot(test_depths.z, label=\"Test\")\nplt.legend()\nplt.title(\"Depth distribution\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b86098c180a1de1ef225db27eb5bbc6b6c6b56bc"},"cell_type":"markdown","source":"A nice gaussian distribution was produced on both the train and testing sets which means we'll get a good range of different sediment layers."},{"metadata":{"_uuid":"b5bc3b212a051ae54d080e099aadc4dc135cc587"},"cell_type":"markdown","source":"## Preprocessing\n\nFirst I'm, going to set the masks to all black for any that have perfectly straight borders as these are definitely not accurate and rather than remove them I will simply set them to a no salt state."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"b6cf5b614f1562fbfcf765d6f280a5bfa4ed1599","collapsed":true},"cell_type":"code","source":"# I use opencv to edge detect the masks\n# As the images are segmented by row I rotate the edge image\n# I check if the row is entirely filled with an edge meaning a perfectly straight edge\ndef has_straight_border(mask_id, shape, edge_array):\n    mask = cv2.imread('../input/train/masks/{}.png'.format(mask_id))\n    edges = cv2.Canny(mask, 100, 200)\n    mat = cv2.getRotationMatrix2D((shape[0]/2,shape[1]/2),90,1)\n    edges_rotated = cv2.warpAffine(edges, mat, shape)\n    edge_array.append(edges_rotated)\n    for row in edges_rotated:\n        if all(pix == 255 for pix in row):\n            return True\n    return False\n\n# Now find the list of straight edged masks\nstraight_borders = []\nedges = []\nfor idx in train_idx:\n    if has_straight_border(idx, image_size, edges):\n        straight_borders.append(idx)\nprint(\"{} masks with straight borders.\".format(len(straight_borders)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f940f87edb5c66d37ebe9ee7b9531a5d315b7b5e"},"cell_type":"markdown","source":"Now I'll have a look at the resulting masks and see if they make sense to remove.\nI'll do this by overlaying the masks onto the images, where the salt is in green and the other sediment is in grey."},{"metadata":{"trusted":true,"_uuid":"518403a7d1b4ae5dc275d1d80fb1712b8ebe4fc0","collapsed":true},"cell_type":"code","source":"max_images = 117\ngrid_width = 9\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*2, grid_height*2))\nfor i, idx in enumerate(straight_borders[:max_images]):\n    mask = train_full.loc[idx].masks\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(train_full.images[np.where(train_idx == idx)[0][0]], cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdd938d11eb1a4c81e71317d33144d90e1e21a4d"},"cell_type":"markdown","source":"There are 90 masks with perfectly straight borders, some of which look to be just inaccurate labelling, others seem to be mostly salt with colums of sediment. I don't want to lose the latter information as we saw in the coverage graph many of the images are quite low salt coverage. To remedy this I am going to keep any data points with a coverage class of 8 and above and drop the rest so as to avoid innaccurate border definition in the model."},{"metadata":{"trusted":true,"_uuid":"fbed8b73d1189f81f2897c0d954af4c40ba11f79","collapsed":true},"cell_type":"code","source":"full_len = len(straight_borders)\nto_be_removed = []\nfor idx in straight_borders:\n    if train_full.coverage_class[idx] < 8:\n        to_be_removed.append(idx)\n        \nprint(\"{}/{} stright bordered data points to be removed:\".format(len(to_be_removed), full_len))\npprint(to_be_removed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88d05351f7d1a11a2835ed7eb00d3c8e11f6bc7e","collapsed":true},"cell_type":"code","source":"train_full = train_full.drop(to_be_removed)\ntrain_idx = train_full.index\nprint(\"New Data Shape: {}\".format(train_full.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6168dc66f05851e78bc7a4ce94bb4475d688cec3"},"cell_type":"markdown","source":"## Creating the Training and Validation Sets\nI'll use the coverage class attribute as a stratification criterion to allow for even distribution in both sets.  \n\nFirst we'll need a function to resize images to be an exponentially divisible number for the Unet model.\nAs well as a function for returning the images back to their original size."},{"metadata":{"trusted":true,"_uuid":"1d9ca3a849b4fc193d2a6d0cc42d3c8eb5c3dda6","collapsed":true},"cell_type":"code","source":"new_size = 128\n\n# Resizes the image to a square with sides the equal to the size provided\ndef upsample(img):\n    if img.shape[0] == new_size:\n        return img\n    return resize(img, (new_size, new_size), mode='constant', preserve_range=True)\n\n# Will need to return images to their original size for post processing\ndef downsample(img):\n    if img.shape[0] == image_size[0]:\n        return img\n    return resize(img, image_size, mode='constant', preserve_range=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a565cb83baca0a8cbff9558ee444a77dccdfe10","collapsed":true},"cell_type":"code","source":"train_id, val_id, train_x, val_x, train_y, val_y, train_cov, test_cov, train_depth, test_depth = train_test_split(\n    train_idx,\n    np.array(train_full.images.map(upsample).tolist()).reshape(-1, new_size, new_size, 1), \n    np.array(train_full.masks.map(upsample).tolist()).reshape(-1, new_size, new_size, 1), \n    train_full.coverage.values,\n    train_full.z.values,\n    test_size=0.2, stratify=train_full.coverage_class, random_state=117)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff5b3dd2c713dc58ac2820290646b8e0aa9d6167","collapsed":true},"cell_type":"code","source":"tmp_img = np.zeros((new_size, new_size), dtype=train_full.images[train_id[25]].dtype)\ntmp_img[:image_size[0], :image_size[1]] = train_full.images[train_id[25]]\nfix, axs = plt.subplots(1, 2, figsize=(15,5))\naxs[0].imshow(tmp_img, cmap=\"Greys\")\naxs[0].set_title(\"Original image\")\naxs[1].imshow(train_x[25].squeeze(), cmap=\"Greys\")\naxs[1].set_title(\"Scaled image\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9ec58c2bce00d44729900b0f9c0a7584af9730a"},"cell_type":"markdown","source":"Looks like the splitting and scaling has worked perfectly.\n\n## Augmenting the Data\n\nI will also augment the data to horizontally flip the images and add these to the current images doubling our training data set as the nature of the images and their non-uniformity allows us to do this without risking overfitting."},{"metadata":{"trusted":true,"_uuid":"52f2802be277ad0b398144a2b2dacf9004611f4f","collapsed":true},"cell_type":"code","source":"train_x = np.append(train_x, [np.fliplr(img) for img in train_x], axis=0)\ntrain_y = np.append(train_y, [np.fliplr(img) for img in train_y], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f514782322820e256400e45022336905922b988f","collapsed":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 10, figsize=(15, 3))\nfor i in range(10):\n    axs[0][i].imshow(train_x[i].squeeze(), cmap=\"Greys\")\n    axs[0][i].imshow(train_y[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n    axs[1][i].imshow(train_x[int(len(train_x)/2 + i)].squeeze(), cmap=\"Greys\")\n    axs[1][i].imshow(train_y[int(len(train_x)/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a584f10afdc0a7a038cf526158375d6709b0ca01"},"cell_type":"markdown","source":"## Building the Model\n\nNow I need to build the Unet model, I'm going to make a function for this so that I can experiment with different amounts of channels. Because of how Unets work I am limited in the number of layers I can create based on the image size."},{"metadata":{"trusted":true,"_uuid":"b26c890d6b1ddb0a16c16be7d7d6e61f9e08df53","collapsed":true},"cell_type":"code","source":"def build_model(input_layer, base_channels):\n    dropout = 0.25\n    kernel = (3, 3)\n    stride = (2, 2)\n    # Image size: 128 to 64\n    conv1 = Conv2D(base_channels, kernel, activation=\"relu\", padding=\"same\")(input_layer)\n    conv1 = Conv2D(base_channels, kernel, activation=\"relu\", padding=\"same\")(conv1)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n    pool1 = Dropout(dropout)(pool1)\n    \n    # Image size: 64 to 32\n    conv2 = Conv2D(base_channels * 2, kernel, activation=\"relu\", padding=\"same\")(pool1)\n    conv2 = Conv2D(base_channels * 2, kernel, activation=\"relu\", padding=\"same\")(conv2)\n    pool2 = MaxPooling2D((2, 2))(conv2)\n    pool2 = Dropout(dropout*2)(pool2)\n    \n    # Image size: 32 to 16\n    conv3 = Conv2D(base_channels * 4, kernel, activation=\"relu\", padding=\"same\")(pool2)\n    conv3 = Conv2D(base_channels * 4, kernel, activation=\"relu\", padding=\"same\")(conv3)\n    pool3 = MaxPooling2D((2, 2))(conv3)\n    pool3 = Dropout(dropout*2)(pool3)\n    \n    # Image size: 16 to 8\n    conv4 = Conv2D(base_channels * 8, kernel, activation=\"relu\", padding=\"same\")(pool3)\n    conv4 = Conv2D(base_channels * 8, kernel, activation=\"relu\", padding=\"same\")(conv4)\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    pool4 = Dropout(dropout*2)(pool4)\n    \n    # Midway point\n    conv_mid = Conv2D(base_channels * 16, kernel, activation=\"relu\", padding=\"same\")(pool4)\n    conv_mid = Conv2D(base_channels * 16, kernel, activation=\"relu\", padding=\"same\")(conv_mid)\n    \n    # Now we mirror the above layers and begin increasing the image size using Conv2DTranspose layers\n    # Image size: 8 to 16\n    trans_conv4 = Conv2DTranspose(base_channels * 8, kernel, strides=stride, padding=\"same\")(conv_mid)\n    uconv4 = concatenate([trans_conv4, conv4])\n    uconv4 = Dropout(dropout * 2)(uconv4)\n    uconv4 = Conv2D(base_channels * 8, kernel, activation=\"relu\", padding=\"same\")(uconv4)\n    uconv4 = Conv2D(base_channels * 8, kernel, activation=\"relu\", padding=\"same\")(uconv4)\n    \n    # Image size: 16 to 32\n    trans_conv3 = Conv2DTranspose(base_channels * 4, kernel, strides=stride, padding=\"same\")(uconv4)\n    uconv3 = concatenate([trans_conv3, conv3])\n    uconv3 = Dropout(dropout * 2)(uconv3)\n    uconv3 = Conv2D(base_channels * 4, kernel, activation=\"relu\", padding=\"same\")(uconv3)\n    uconv3 = Conv2D(base_channels * 4, kernel, activation=\"relu\", padding=\"same\")(uconv3)\n    \n    # Image size: 32 to 64\n    trans_conv2 = Conv2DTranspose(base_channels * 2, kernel, strides=stride, padding=\"same\")(uconv3)\n    uconv2 = concatenate([trans_conv2, conv2])\n    uconv2 = Dropout(dropout * 2)(uconv2)\n    uconv2 = Conv2D(base_channels * 2, kernel, activation=\"relu\", padding=\"same\")(uconv2)\n    uconv2 = Conv2D(base_channels * 2, kernel, activation=\"relu\", padding=\"same\")(uconv2)\n    \n    # Image size: 64 to 128\n    trans_conv1 = Conv2DTranspose(base_channels, kernel, strides=stride, padding=\"same\")(uconv2)\n    uconv1 = concatenate([trans_conv1, conv1])\n    uconv1 = Dropout(dropout)(uconv1)\n    uconv1 = Conv2D(base_channels, kernel, activation=\"relu\", padding=\"same\")(uconv1)\n    uconv1 = Conv2D(base_channels, kernel, activation=\"relu\", padding=\"same\")(uconv1)\n    \n    output_layer = Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n    \n    return output_layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79d4e49e79d49fa233cc0f47e5de03a0658521e9","collapsed":true},"cell_type":"code","source":"input_layer = Input((new_size, new_size, 1))\noutput_layer = build_model(input_layer, 16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a897f9a7868bae2c10554f415247b5956974e31","collapsed":true},"cell_type":"code","source":"model = Model(input_layer, output_layer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa6f0b0e2881b2c810491f84f443f0898b3974d1","collapsed":true},"cell_type":"code","source":"model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e67b5d55e262c4598c48bbd8ef769c945ee4ef3c","collapsed":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a066e2b27ed5171b2c5a14740867484c94963de9"},"cell_type":"markdown","source":"## Training the Model\n\nNow I can begin training the model, I am unsure what parameters I will start out with but I will experiment with each to find what works best.\n\nI know that my setup will be a large number of epochs so that I can use early stopping to find the best possible model. I will store the best model in a seperate file for later use."},{"metadata":{"trusted":true,"_uuid":"9fd9e3480cb8c2aeb8188e7862ffb8938214cb91","scrolled":true,"collapsed":true},"cell_type":"code","source":"early_stop = EarlyStopping(patience=10, verbose=1)\nmodel_check = ModelCheckpoint(\"./keras.model\", save_best_only=True, verbose=1)\nlearning_rate_control = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)\nepochs = 100\nbatch_size = 32\n\nresults = model.fit(train_x, train_y,\n                    validation_data=[val_x, val_y],\n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[early_stop, model_check, learning_rate_control])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cac66bf82ade88d81f70cf95f308086fb5fef77a"},"cell_type":"markdown","source":"Now let's have a look at how the training loss and the validation loss values compare as well as the training and validation accuracy.  \nWe should be able to easily see the point at which the two begin to diverge which is where our best model lies."},{"metadata":{"trusted":true,"_uuid":"6eece00d7016b541ff23b518c35cff11e651f3fb","collapsed":true},"cell_type":"code","source":"fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(results.epoch, results.history[\"loss\"], label=\"Train loss\")\nax_loss.plot(results.epoch, results.history[\"val_loss\"], label=\"Validation loss\")\nax_acc.plot(results.epoch, results.history[\"acc\"], label=\"Train accuracy\")\nax_acc.plot(results.epoch, results.history[\"val_acc\"], label=\"Validation accuracy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4431ab9cc905c4713de84ab99e893cc208787f06","collapsed":true},"cell_type":"code","source":"model = load_model(\"./keras.model\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"861a079cca095ef14a3b43858e852a4094b29ce3"},"cell_type":"markdown","source":"## Sanity Check using Validation Set\nI'll have the model predict the validation set and overlay the prediction onto the images and masks to see how they match up."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2d4aae1383f7b0b02f4a0152f752de3158432db8"},"cell_type":"code","source":"val_predictions = model.predict(val_x).reshape(-1, new_size, new_size)\nval_predictions = np.array([downsample(x) for x in val_predictions])\nval_y_original = np.array([train_full.masks[idx] for idx in val_id])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2cbfcf36a84c8d7ca7668d91240779c5d6263b1","collapsed":true},"cell_type":"code","source":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(val_id[:max_images]):\n    img = train_full.images[idx]\n    mask = train_full.masks[idx]\n    pred = val_predictions[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1d21c6c63b6f0ee95c90975a9543b341e191ca66"},"cell_type":"markdown","source":"The intensity of the red represents the probability that the pixel is salt as predicted by the model.\nSome of the images have been predicted erfectly with exact borders between salt and sediment, some images with large sections of salt the model struggles to register all of it. This leads me to believe the model has perhaps trained to recognise the border quite well rather than learning to recognise the salt itself.\nThe fuzzier edges and sections are where the model predicts a low probablility of salt.  \n\nNow we'll run the IoU metrics with each provided threshold and graph the score received at each threshold."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"84c38c606076155ae858ed7d72b190d063bf92b0"},"cell_type":"code","source":"# src: https://www.kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7121eb3f7bb3ee3c4791a7b80c9ff6aa0c3d01de","collapsed":true},"cell_type":"code","source":"thresholds = np.linspace(0, 1, 50)\nious = np.array([iou_metric_batch(val_y_original, np.int32(val_predictions > threshold)) for threshold in tqdm_notebook(thresholds)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"26f5fe3bdf4063f9f37e92fbc3722a49a59d2d62"},"cell_type":"code","source":"threshold_best_index = np.argmax(ious[9:-10]) + 9\niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ff0ccf6f663a23f9f6ba8e99f41045e8a626e5b","collapsed":true},"cell_type":"code","source":"plt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6eaad360ba7c3420c0eb0e9fb679354ad838f0fa"},"cell_type":"markdown","source":"The graph shows a wide curve where the low threesholds produces a low scre due to very low probability pixels are counted as salt predictions.  \nThe high thresholds also score lower as only pixels the model is very sure of are counted and so this leaves very little room for error.\nAs we can see the model does best when the threshold is set to only count pixels as salt predictions with an IoU of ~0.4 -> 0.7.\nThe goal is to make this curve as tall and as wide as possible since the end metric is the average of all these scores."},{"metadata":{"trusted":true,"_uuid":"74733f5d4ff006f034337e23de333de2b0082dc2","collapsed":true},"cell_type":"code","source":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(val_id[:max_images]):\n    img = train_full.images[idx]\n    mask = train_full.masks[idx]\n    pred = val_predictions[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.3, cmap=\"OrRd\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6f2bce8602501f28167414490b3b57e9033ef1e"},"cell_type":"markdown","source":"With the best scoring threshold applied we can see that the fuzzy edges are significantly reduced, resulting in sharper, more clear predictions.\n\n## Scoring\nNow to create the submission file, ads this is a competition with over a thousand participants there was no need to reinvent the wheel and write my own functions for this, I am using the functions created by user bguberfain, link to the source is in the code."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"64a62ef830f42c827bee010e3c8252be39a3f39e"},"cell_type":"code","source":"# Source https://www.kaggle.com/bguberfain/unet-with-depth\ndef RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ada4dc62b001643d304896909762b079a4043591"},"cell_type":"markdown","source":"Now to load and predict the test set, and create the submission file."},{"metadata":{"trusted":true,"_uuid":"d36c9a3c3bee53dac9b7325d6c3eed3f629d1114","collapsed":true},"cell_type":"code","source":"x_test = np.array([upsample(np.array(load_img(\"../input/test/images/{}.png\".format(idx), grayscale=True))) / 255 for idx in tqdm_notebook(test_depths.index)]).reshape(-1, new_size, new_size, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7a5a5690f2a81dea2f87410256146da02a14fdc0"},"cell_type":"code","source":"preds_test = model.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cd60dcf8a5b8e3a3d92e72858f7ad1465c04162","collapsed":true},"cell_type":"code","source":"pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_depths.index.values))}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb216f697818e978e60ffbb6ef1308dd52b4d991","collapsed":true},"cell_type":"code","source":"sub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1ea31652f4e983361aaaba000b2549c49b2609eb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}