{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "06dc531a8cb411334161905e4df621ffc9705a28"
   },
   "source": [
    "# Salt Identification Challenge\n",
    "## Capstone Project\n",
    "Shane Moloney  \n",
    "Udacity Machine Learning Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1575855f590cea126c825565a3f8b9849cfaf4e"
   },
   "source": [
    "## Set up\n",
    "\n",
    "Imports for all the project dependencies.\n",
    "I may split this so that the dependencies are in the relevant code cells to avoid needing to run this every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e33c6960a4d6edf66dc23a3728f14a1049086648"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "95b583d4945e07a96b93c038fb072a3842fc8e56"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smoloney/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import cv2\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d4951504395c322ecfed8e96a3e16755ce28612e"
   },
   "source": [
    "### Load in a Sample of the data\n",
    "\n",
    "Let's have a look at the data to see what will need to be done to prepare it for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "97e6460a36278ee0784cec30c370658145f0cd0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     rle_mask\n",
      "id                                                           \n",
      "575d24d81d                                                NaN\n",
      "a266a2a9df                                          5051 5151\n",
      "75efad62c1  9 93 109 94 210 94 310 95 411 95 511 96 612 96...\n",
      "34e51dba6a  48 54 149 54 251 53 353 52 455 51 557 50 659 4...\n",
      "4875705fb0  1111 1 1212 1 1313 1 1414 1 1514 2 1615 2 1716...\n",
      "              z\n",
      "id             \n",
      "4ac19fb269  306\n",
      "1825fadf99  157\n",
      "f59821d067  305\n",
      "5b435fad9d  503\n",
      "e340e7bfca  783\n"
     ]
    }
   ],
   "source": [
    "train_samp = pd.read_csv(\"data/train/train.csv\", index_col=\"id\", nrows=5)\n",
    "depths_samp = pd.read_csv(\"data/depths.csv\", index_col=\"id\", nrows=5)\n",
    "print(train_samp)\n",
    "print(depths_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7918c3bb563f191728dc8be7a7600db8f51ca537"
   },
   "source": [
    "Above we can see the run-length encoded masks and the depths of the first five images.  \n",
    "\n",
    "As we can see, images with no salt deposits have a null value and images with deposits have the pixels listed as ranges that cantain salt.  \n",
    "\n",
    "Now we will look at the images themselves along with their provided masks as these are kindly provided to us in png form although they could also be extracted from the train.csv file by decoding the entries in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b60c453b64744b74e1f2ae693e3c90a198e5fa82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Dimensions: (101, 101)\n"
     ]
    }
   ],
   "source": [
    "train_inds = train_samp.index\n",
    "\n",
    "image_samp = [np.array(load_img(\"data/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in train_inds]\n",
    "mask_samp = [np.array(load_img(\"data/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in train_inds]\n",
    "image_size = image_samp[0].shape\n",
    "print(\"Image Dimensions: {}\".format(image_size))\n",
    "\n",
    "fig, axs = plt.subplots(5, 2, figsize=(10, 25))\n",
    "for i in range(0, 5):\n",
    "    axs[int(i)][0].imshow(image_samp[i-1], cmap='gray')\n",
    "    axs[int(i)][1].imshow(mask_samp[i-1], cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a3737467e198ca86272417a2123979d2cebda31b"
   },
   "source": [
    "We can see that some masks may not be accurate as in the figure above the mask seems to claim that the salt deposit takes up exactly half the image with a perfectly straight border, which we can clearly see is incorrect.  \n",
    "\n",
    "I may decide to remove any data whose mask matches this pattern.  \n",
    "\n",
    "For now lets load in all the data and start looking at it as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d0597058dc5f1738582bf4d4ce7c356c4cbaa641"
   },
   "source": [
    "## Loading in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "853c5e18b41f39079870dfb8f6e9c4c89c7ac34a"
   },
   "source": [
    "First we'll import the CSV files and add the depths as well and store a list of the indeces and a list of the test set depths, ie the depths whose ID's do not appear in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d2d41d32f1b8a21f185b66bd7756c1c27271896a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_full = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\n",
    "depths = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\n",
    "\n",
    "train_full = train_full.join(depths)\n",
    "train_idx = train_full.index\n",
    "\n",
    "test_depths = depths[~depths.index.isin(train_full.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aba0bfce466e7b7d53564b4852b0124f98946654"
   },
   "source": [
    "\n",
    "\n",
    "Now we import the image and mask files, 4000 in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cbde9fcc69deab102739379c15c9e28919cd599f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_full[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69ecf86245987b47b5950818ea5b89be9c358ec7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_full[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_idx)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9106b4bb1b09a89bd0377ddcabbe2d5cbbad104a"
   },
   "source": [
    "Create a column in the array for the coverage of the salt in each image. This means the number of pixels xcontaining salt over the area of the image. As this can be any decimal between 0 and 1 we discretise these values with the cov_to_class function which gives it an integer class from 1 to 10 which represent no salt up to only salt respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "671c24d495159f45bc8df32cc0cf7156eb2d40c0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_full[\"coverage\"] = train_full.masks.map(np.sum) / pow(image_size[0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4304cc595d680a096bfc314722c7df02bd8939c4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cov_to_class(val):    \n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i :\n",
    "            return i\n",
    "        \n",
    "train_full[\"coverage_class\"] = train_full.coverage.map(cov_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2ff24b630c56b5ea6feeb00da013d6f0e3be0696",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "sns.distplot(train_full.coverage, kde=False, ax=axs[0])\n",
    "sns.distplot(train_full.coverage_class, bins=10, kde=False, ax=axs[1])\n",
    "plt.suptitle(\"Salt coverage\")\n",
    "axs[0].set_xlabel(\"Coverage\")\n",
    "axs[1].set_xlabel(\"Coverage class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "18339b8a9ef26fa33a114a3743e5e6097b5f660f"
   },
   "source": [
    "We can see that a large amount of the images have no or very little salt, half the data set resides in classes 1 and 2. This is good as the model needs to know that there are images without salt as well.\n",
    "\n",
    "## Depths\n",
    "Now let's have a look at the depths and how good their random sampling was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e134c09e00464861dd217c4b105f713c5a69d683",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(train_full.z, label=\"Train\")\n",
    "sns.distplot(test_depths, label=\"Test\")\n",
    "plt.legend()\n",
    "plt.title(\"Depth distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b86098c180a1de1ef225db27eb5bbc6b6c6b56bc"
   },
   "source": [
    "A nice gaussian distribution was produced on both the train and testing sets which means we'll get a good range of different sediment layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b5bc3b212a051ae54d080e099aadc4dc135cc587"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "First I'm, going to set the masks to all black for any that have perfectly straight borders as these are definitely not accurate and rather than remove them I will simply set them to a no salt state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b6cf5b614f1562fbfcf765d6f280a5bfa4ed1599",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I use opencv to edge detect the masks\n",
    "# As the images are segmented by row I rotate the edge image\n",
    "# I check if the row is entirely filled with an edge meaning a perfectly straight edge\n",
    "def has_straight_border(mask_id, shape, edge_array):\n",
    "    mask = cv2.imread('../input/train/masks/{}.png'.format(mask_id))\n",
    "    edges = cv2.Canny(mask, 100, 200)\n",
    "    mat = cv2.getRotationMatrix2D((shape[0]/2,shape[1]/2),90,1)\n",
    "    edges_rotated = cv2.warpAffine(edges, mat, shape)\n",
    "    edge_array.append(edges_rotated)\n",
    "    for row in edges_rotated:\n",
    "        if all(pix == 255 for pix in row):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Now find the list of straight edged masks\n",
    "straight_borders = []\n",
    "edges = []\n",
    "for idx in train_idx:\n",
    "    if has_straight_border(idx, image_size, edges):\n",
    "        straight_borders.append(idx)\n",
    "print(\"{} masks with straight borders.\".format(len(straight_borders)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f940f87edb5c66d37ebe9ee7b9531a5d315b7b5e"
   },
   "source": [
    "Now I'll have a look at the resulting masks and see if they make sense to remove.\n",
    "I'll do this by overlaying the masks onto the images, where the salt is in green and the other sediment is in grey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "518403a7d1b4ae5dc275d1d80fb1712b8ebe4fc0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_images = 117\n",
    "grid_width = 9\n",
    "grid_height = int(max_images / grid_width)\n",
    "fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*2, grid_height*2))\n",
    "for i, idx in enumerate(straight_borders[:max_images]):\n",
    "    mask = train_full.loc[idx].masks\n",
    "    ax = axs[int(i / grid_width), i % grid_width]\n",
    "    ax.imshow(train_full.images[np.where(train_idx == idx)[0][0]], cmap=\"Greys\")\n",
    "    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cdd938d11eb1a4c81e71317d33144d90e1e21a4d"
   },
   "source": [
    "There are 90 masks with perfectly straight borders, some of which look to be just inaccurate labelling, others seem to be mostly salt with colums of sediment. I don't want to lose the latter information as we saw in the coverage graph many of the images are quite low salt coverage. To remedy this I am going to keep any data points with a coverage class of 8 and above and drop the rest so as to avoid innaccurate border definition in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fbed8b73d1189f81f2897c0d954af4c40ba11f79",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_len = len(straight_borders)\n",
    "to_be_removed = []\n",
    "for idx in straight_borders:\n",
    "    if train_full.coverage_class[idx] < 8:\n",
    "        to_be_removed.append(idx)\n",
    "        \n",
    "print(\"{}/{} stright bordered data points to be removed:\".format(len(to_be_removed), full_len))\n",
    "pprint(to_be_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "88d05351f7d1a11a2835ed7eb00d3c8e11f6bc7e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_full = train_full.drop(to_be_removed)\n",
    "train_idx = train_full.index\n",
    "print(\"New Data Shape: {}\".format(train_full.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6168dc66f05851e78bc7a4ce94bb4475d688cec3"
   },
   "source": [
    "## Creating the Training and Validation Sets\n",
    "I'll use the coverage class attribute as a stratification criterion to allow for even distribution in both sets.  \n",
    "\n",
    "First we'll need a function to resize images to be an exponentially divisible number for the Unet model.\n",
    "As well as a function for returning the images back to their original size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1d9ca3a849b4fc193d2a6d0cc42d3c8eb5c3dda6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_size = 128\n",
    "\n",
    "# Resizes the image to a square with sides the equal to the size provided\n",
    "def upsample(img):\n",
    "    if img.shape[0] == new_size:\n",
    "        return img\n",
    "    return resize(img, (new_size, new_size), mode='constant', preserve_range=True)\n",
    "\n",
    "# Will need to return images to their original size for post processing\n",
    "def downsample(img):\n",
    "    if img.shape[0] == image_size[0]:\n",
    "        return img\n",
    "    return resize(img, image_size, mode='constant', preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a565cb83baca0a8cbff9558ee444a77dccdfe10",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_id, val_id, train_x, val_x, train_y, val_y, train_cov, test_cov, train_cov_class, test_cov_class, train_depth, test_depth = train_test_split(\n",
    "    train_idx,\n",
    "    np.array(train_full.images.map(upsample).tolist()).reshape(-1, new_size, new_size, 1), \n",
    "    np.array(train_full.masks.map(upsample).tolist()).reshape(-1, new_size, new_size, 1), \n",
    "    train_full.coverage.values,\n",
    "    train_full.coverage_class.values,\n",
    "    train_full.z.values,\n",
    "    test_size=0.2, stratify=train_full.coverage_class, random_state=117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ff5b3dd2c713dc58ac2820290646b8e0aa9d6167",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_img = np.zeros((new_size, new_size), dtype=train_full.images[train_id[25]].dtype)\n",
    "tmp_img[:image_size[0], :image_size[1]] = train_full.images[train_id[25]]\n",
    "fix, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "axs[0].imshow(tmp_img, cmap=\"Greys\")\n",
    "axs[0].set_title(\"Original image\")\n",
    "axs[1].imshow(train_x[25].squeeze(), cmap=\"Greys\")\n",
    "axs[1].set_title(\"Scaled image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9ec58c2bce00d44729900b0f9c0a7584af9730a"
   },
   "source": [
    "Looks like the splitting and scaling has worked perfectly.\n",
    "\n",
    "## Augmenting the Data\n",
    "\n",
    "I will also augment the data to horizontally flip the images and add these to the current images doubling our training data set as the nature of the images and their non-uniformity allows us to do this without risking overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "52f2802be277ad0b398144a2b2dacf9004611f4f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = np.append(train_x, [np.fliplr(img) for img in train_x], axis=0)\n",
    "train_y = np.append(train_y, [np.fliplr(img) for img in train_y], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f514782322820e256400e45022336905922b988f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 10, figsize=(15, 3))\n",
    "for i in range(10):\n",
    "    axs[0][i].imshow(train_x[i].squeeze(), cmap=\"Greys\")\n",
    "    axs[0][i].imshow(train_y[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n",
    "    axs[1][i].imshow(train_x[int(len(train_x)/2 + i)].squeeze(), cmap=\"Greys\")\n",
    "    axs[1][i].imshow(train_y[int(len(train_x)/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a584f10afdc0a7a038cf526158375d6709b0ca01"
   },
   "source": [
    "## Building the Model\n",
    "\n",
    "Now I need to build the Unet model, I'm going to make a function for this so that I can experiment with different amounts of channels. Because of how Unets work I am limited in the number of layers I can create based on the image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b26c890d6b1ddb0a16c16be7d7d6e61f9e08df53",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_layer, base_channels):\n",
    "    dropout = 0.25\n",
    "    kernel = (3, 3)\n",
    "    stride = (2, 2)\n",
    "    # Image size: 128 to 64\n",
    "    conv1 = Conv2D(base_channels, kernel, activation=\"relu\", padding=\"same\")(input_layer)\n",
    "    conv1 = Conv2D(base_channels, kernel, activation=\"relu\", padding=\"same\")(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    pool1 = Dropout(dropout)(pool1)\n",
    "    \n",
    "    # Image size: 64 to 32\n",
    "    conv2 = Conv2D(base_channels * 2, kernel, activation=\"relu\", padding=\"same\")(pool1)\n",
    "    conv2 = Conv2D(base_channels * 2, kernel, activation=\"relu\", padding=\"same\")(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    pool2 = Dropout(dropout*2)(pool2)\n",
    "    \n",
    "    # Image size: 32 to 16\n",
    "    conv3 = Conv2D(base_channels * 4, kernel, activation=\"relu\", padding=\"same\")(pool2)\n",
    "    conv3 = Conv2D(base_channels * 4, kernel, activation=\"relu\", padding=\"same\")(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "    pool3 = Dropout(dropout*2)(pool3)\n",
    "    \n",
    "    # Image size: 16 to 8\n",
    "    conv4 = Conv2D(base_channels * 8, kernel, activation=\"relu\", padding=\"same\")(pool3)\n",
    "    conv4 = Conv2D(base_channels * 8, kernel, activation=\"relu\", padding=\"same\")(conv4)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    pool4 = Dropout(dropout*2)(pool4)\n",
    "    \n",
    "    # Midway point\n",
    "    conv_mid = Conv2D(base_channels * 16, kernel, activation=\"relu\", padding=\"same\")(pool4)\n",
    "    conv_mid = Conv2D(base_channels * 16, kernel, activation=\"relu\", padding=\"same\")(conv_mid)\n",
    "    \n",
    "    # Now we mirror the above layers and begin increasing the image size using Conv2DTranspose layers\n",
    "    # Image size: 8 to 16\n",
    "    trans_conv4 = Conv2DTranspose(base_channels * 8, kernel, strides=stride, padding=\"same\")(conv_mid)\n",
    "    uconv4 = concatenate([trans_conv4, conv4])\n",
    "    uconv4 = Dropout(dropout * 2)(uconv4)\n",
    "    uconv4 = Conv2D(base_channels * 8, kernel, activation=\"relu\", padding=\"same\")(uconv4)\n",
    "    uconv4 = Conv2D(base_channels * 8, kernel, activation=\"relu\", padding=\"same\")(uconv4)\n",
    "    \n",
    "    # Image size: 16 to 32\n",
    "    trans_conv3 = Conv2DTranspose(base_channels * 4, kernel, strides=stride, padding=\"same\")(uconv4)\n",
    "    uconv3 = concatenate([trans_conv3, conv3])\n",
    "    uconv3 = Dropout(dropout * 2)(uconv3)\n",
    "    uconv3 = Conv2D(base_channels * 4, kernel, activation=\"relu\", padding=\"same\")(uconv3)\n",
    "    uconv3 = Conv2D(base_channels * 4, kernel, activation=\"relu\", padding=\"same\")(uconv3)\n",
    "    \n",
    "    # Image size: 32 to 64\n",
    "    trans_conv2 = Conv2DTranspose(base_channels * 2, kernel, strides=stride, padding=\"same\")(uconv3)\n",
    "    uconv2 = concatenate([trans_conv2, conv2])\n",
    "    uconv2 = Dropout(dropout * 2)(uconv2)\n",
    "    uconv2 = Conv2D(base_channels * 2, kernel, activation=\"relu\", padding=\"same\")(uconv2)\n",
    "    uconv2 = Conv2D(base_channels * 2, kernel, activation=\"relu\", padding=\"same\")(uconv2)\n",
    "    \n",
    "    # Image size: 64 to 128\n",
    "    trans_conv1 = Conv2DTranspose(base_channels, kernel, strides=stride, padding=\"same\")(uconv2)\n",
    "    uconv1 = concatenate([trans_conv1, conv1])\n",
    "    uconv1 = Dropout(dropout)(uconv1)\n",
    "    uconv1 = Conv2D(base_channels, kernel, activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    uconv1 = Conv2D(base_channels, kernel, activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    \n",
    "    output_layer = Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "79d4e49e79d49fa233cc0f47e5de03a0658521e9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = Input((new_size, new_size, 1))\n",
    "output_layer = build_model(input_layer, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a897f9a7868bae2c10554f415247b5956974e31",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(input_layer, output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa6f0b0e2881b2c810491f84f443f0898b3974d1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e67b5d55e262c4598c48bbd8ef769c945ee4ef3c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a066e2b27ed5171b2c5a14740867484c94963de9"
   },
   "source": [
    "## Training the Model\n",
    "\n",
    "Now I can begin training the model, I am unsure what parameters I will start out with but I will experiment with each to find what works best.\n",
    "\n",
    "I know that my setup will be a large number of epochs so that I can use early stopping to find the best possible model. I will store the best model in a seperate file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9fd9e3480cb8c2aeb8188e7862ffb8938214cb91",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(patience=10, verbose=1)\n",
    "model_check = ModelCheckpoint(\"./keras.model\", save_best_only=True, verbose=1)\n",
    "learning_rate_control = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "results = model.fit(train_x, train_y,\n",
    "                    validation_data=[val_x, val_y],\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stop, model_check, learning_rate_control])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cac66bf82ade88d81f70cf95f308086fb5fef77a"
   },
   "source": [
    "Now let's have a look at how the training loss and the validation loss values compare as well as the training and validation accuracy.  \n",
    "We should be able to easily see the point at which the two begin to diverge which is where our best model lies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6eece00d7016b541ff23b518c35cff11e651f3fb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax_loss.plot(results.epoch, results.history[\"loss\"], label=\"Train loss\")\n",
    "ax_loss.plot(results.epoch, results.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax_acc.plot(results.epoch, results.history[\"acc\"], label=\"Train accuracy\")\n",
    "ax_acc.plot(results.epoch, results.history[\"val_acc\"], label=\"Validation accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4431ab9cc905c4713de84ab99e893cc208787f06",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(\"./keras.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "861a079cca095ef14a3b43858e852a4094b29ce3"
   },
   "source": [
    "## Sanity Check using Validation Set\n",
    "I'll have the model predict the validation set and overlay the prediction onto the images and masks to see how they match up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2d4aae1383f7b0b02f4a0152f752de3158432db8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_preds = model.predict(val_x).reshape(-1, new_size, new_size)\n",
    "val_predictions = np.array([downsample(x) for x in val_preds])\n",
    "val_y_original = np.array([train_full.masks[idx] for idx in val_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2cbfcf36a84c8d7ca7668d91240779c5d6263b1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_images = 60\n",
    "grid_width = 15\n",
    "grid_height = int(max_images / grid_width)\n",
    "fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n",
    "for i, idx in enumerate(val_id[:max_images]):\n",
    "    img = train_full.images[idx]\n",
    "    mask = train_full.masks[idx]\n",
    "    pred = val_predictions[i]\n",
    "    ax = axs[int(i / grid_width), i % grid_width]\n",
    "    ax.imshow(img, cmap=\"Greys\")\n",
    "    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n",
    "    ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "plt.suptitle(\"Green: salt, Red: prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "11eddc482016dba07538991fb4381f2d847305d1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_predictions[\"coverage\"] = val_predictions.map(np.sum) / pow(image_size[0], 2)\n",
    "val_predictions[\"coverage_class\"] = val_predictions.coverage.map(cov_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "633ae1e7ce9771f3c30bf72aa285079af81d42e4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "sns.distplot(val_predictions.coverage, kde=False, ax=axs[0])\n",
    "sns.distplot(test_cov, kde=False, ax=axs[0])\n",
    "sns.distplot(val_predictions.coverage_class, bins=10, kde=False, ax=axs[1])\n",
    "sns.distplot(test_cov_class, bins=10, kde=False, ax=axs[1])\n",
    "plt.suptitle(\"Prediction vs Ground Truth Salt coverage\")\n",
    "axs[0].set_xlabel(\"Coverage\")\n",
    "axs[1].set_xlabel(\"Coverage class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1d21c6c63b6f0ee95c90975a9543b341e191ca66",
    "collapsed": true
   },
   "source": [
    "The intensity of the red represents the probability that the pixel is salt as predicted by the model.\n",
    "Some of the images have been predicted erfectly with exact borders between salt and sediment, some images with large sections of salt the model struggles to register all of it. This leads me to believe the model has perhaps trained to recognise the border quite well rather than learning to recognise the salt itself.\n",
    "The fuzzier edges and sections are where the model predicts a low probablility of salt.  \n",
    "\n",
    "Now we'll run the IoU metrics with each provided threshold and graph the score received at each threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "84c38c606076155ae858ed7d72b190d063bf92b0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# src: https://www.kaggle.com/aglotero/another-iou-metric\n",
    "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
    "    labels = y_true_in\n",
    "    y_pred = y_pred_in\n",
    "    \n",
    "    true_objects = 2\n",
    "    pred_objects = 2\n",
    "\n",
    "    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n",
    "\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    area_true = np.histogram(labels, bins = true_objects)[0]\n",
    "    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    union = union[1:,1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        matches = iou > threshold\n",
    "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "    \n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)\n",
    "\n",
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n",
    "        metric.append(value)\n",
    "    return np.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7121eb3f7bb3ee3c4791a7b80c9ff6aa0c3d01de",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 50)\n",
    "ious = np.array([iou_metric_batch(val_y_original, np.int32(val_predictions > threshold)) for threshold in tqdm_notebook(thresholds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "26f5fe3bdf4063f9f37e92fbc3722a49a59d2d62",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold_best_index = np.argmax(ious[9:-10]) + 9\n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ff0ccf6f663a23f9f6ba8e99f41045e8a626e5b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(thresholds, ious)\n",
    "plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6eaad360ba7c3420c0eb0e9fb679354ad838f0fa",
    "collapsed": true
   },
   "source": [
    "The graph shows a wide curve where the low threesholds produces a low scre due to very low probability pixels are counted as salt predictions.  \n",
    "The high thresholds also score lower as only pixels the model is very sure of are counted and so this leaves very little room for error.\n",
    "As we can see the model does best when the threshold is set to only count pixels as salt predictions with an IoU of ~0.4 -> 0.7.\n",
    "The goal is to make this curve as tall and as wide as possible since the end metric is the average of all these scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "74733f5d4ff006f034337e23de333de2b0082dc2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_images = 60\n",
    "grid_width = 15\n",
    "grid_height = int(max_images / grid_width)\n",
    "fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n",
    "for i, idx in enumerate(val_id[:max_images]):\n",
    "    img = train_full.images[idx]\n",
    "    mask = train_full.masks[idx]\n",
    "    pred = val_predictions[i]\n",
    "    ax = axs[int(i / grid_width), i % grid_width]\n",
    "    ax.imshow(img, cmap=\"Greys\")\n",
    "    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n",
    "    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.3, cmap=\"OrRd\")\n",
    "    ax.set_yticklabels([]\n",
    "    ax.set_xticklabels([])\n",
    "plt.suptitle(\"Green: salt, Red: prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d62cf58f5cb0f7addef54548c5d0e4c92cb269a4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "sns.distplot(val_predictions.coverage, kde=False, ax=axs[0])\n",
    "sns.distplot(test_cov, kde=False, ax=axs[0])\n",
    "sns.distplot(val_predictions.coverage_class, bins=10, kde=False, ax=axs[1])\n",
    "sns.distplot(test_cov_class, bins=10, kde=False, ax=axs[1])\n",
    "plt.suptitle(\"Prediction vs Ground Truth Salt coverage\")\n",
    "axs[0].set_xlabel(\"Coverage\")\n",
    "axs[1].set_xlabel(\"Coverage class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c6f2bce8602501f28167414490b3b57e9033ef1e"
   },
   "source": [
    "With the best scoring threshold applied we can see that the fuzzy edges are significantly reduced, resulting in sharper, more clear predictions.\n",
    "\n",
    "## Scoring\n",
    "Now to create the submission file, ads this is a competition with over a thousand participants there was no need to reinvent the wheel and write my own functions for this, I am using the functions created by user bguberfain, link to the source is in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "64a62ef830f42c827bee010e3c8252be39a3f39e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source https://www.kaggle.com/bguberfain/unet-with-depth\n",
    "def RLenc(img, order='F', format=True):\n",
    "    \"\"\"\n",
    "    img is binary mask image, shape (r,c)\n",
    "    order is down-then-right, i.e. Fortran\n",
    "    format determines if the order needs to be preformatted (according to submission rules) or not\n",
    "\n",
    "    returns run length as an array or string (if format is True)\n",
    "    \"\"\"\n",
    "    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
    "    runs = []  ## list of run lengths\n",
    "    r = 0  ## the current run length\n",
    "    pos = 1  ## count starts from 1 per WK\n",
    "    for c in bytes:\n",
    "        if (c == 0):\n",
    "            if r != 0:\n",
    "                runs.append((pos, r))\n",
    "                pos += r\n",
    "                r = 0\n",
    "            pos += 1\n",
    "        else:\n",
    "            r += 1\n",
    "\n",
    "    # if last run is unsaved (i.e. data ends with 1)\n",
    "    if r != 0:\n",
    "        runs.append((pos, r))\n",
    "        pos += r\n",
    "        r = 0\n",
    "\n",
    "    if format:\n",
    "        z = ''\n",
    "\n",
    "        for rr in runs:\n",
    "            z += '{} {} '.format(rr[0], rr[1])\n",
    "        return z[:-1]\n",
    "    else:\n",
    "        return runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ada4dc62b001643d304896909762b079a4043591"
   },
   "source": [
    "Now to load and predict the test set, and create the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d36c9a3c3bee53dac9b7325d6c3eed3f629d1114",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.array([upsample(np.array(load_img(\"../input/test/images/{}.png\".format(idx), grayscale=True))) / 255 for idx in tqdm_notebook(test_depths.index)]).reshape(-1, new_size, new_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a5a5690f2a81dea2f87410256146da02a14fdc0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_test = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2cd60dcf8a5b8e3a3d92e72858f7ad1465c04162",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_depths.index.values))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cb216f697818e978e60ffbb6ef1308dd52b4d991",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame.from_dict(pred_dict,orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "sub.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1ea31652f4e983361aaaba000b2549c49b2609eb",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
